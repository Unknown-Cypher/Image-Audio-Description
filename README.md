# Image-Audio-Description
This project's aim to understand the image which is fed to it, recognise and give an audio description. This project is targetted towards blind people. The blind select which image is to be read in the local hosted website, the image then gets converted to a set of features which is then fed into a neural network that forms a sentence upon analysing the features. Then we use pyttsx module to convert the obtained sentence to an audio clip. The audio clip is played tso that the blind can undertsand what the picture is trying to potray.

# Features Implemented-
Feature extraction from images
Establishing sensible sentences after analysing the features
converion of sentences to an audio clip

# Tech stack/concepts used-
1. Deep Learning
2. Convulutional Neural Networks
3. Recuurent Neural Networks
4. Word Embeddings
5. pyttsx module
6. Tensorflow module
7. Keras module

# Thought behind the project-
We were looking for an oppurtunity to improve our machine learning and deep learning skills when we came across this project. This project was well suited for us as this project involves solving a real life problem using machine learning. We advanced into this project with a goal that we could help the bling people to perceive images around them. We are happy to put our skills to use to help the needed. This project helped us to expect how the real life problems could be solved uusing machine learning.

# Team Members-
1. Gumidelli Chandrahas
2. Avantsa Raja Yashwanth

